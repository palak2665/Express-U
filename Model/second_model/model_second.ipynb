{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential,Model\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dropout,Dense,ReLU,Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r'D:\\Final Year Project\\Express-U\\data\\live_dataset\\test'\n",
    "test_path = r'D:\\Final Year Project\\Express-U\\data\\live_dataset\\train' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1435 images belonging to 35 classes.\n",
      "Found 24535 images belonging to 35 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function= tensorflow.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "model.add(Dense(35,activation =\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 15, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                294976    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 35)                4515      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 417,571\n",
      "Trainable params: 417,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "144/144 [==============================] - 138s 963ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 1.0804 - val_accuracy: 0.7502 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "144/144 [==============================] - 37s 258ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.0426 - val_accuracy: 0.7649 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "144/144 [==============================] - 33s 232ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.0396 - val_accuracy: 0.7712 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "144/144 [==============================] - 32s 227ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.0301 - val_accuracy: 0.7786 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "144/144 [==============================] - 33s 230ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.0207 - val_accuracy: 0.7828 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "144/144 [==============================] - 33s 227ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.0184 - val_accuracy: 0.7859 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "144/144 [==============================] - 33s 229ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.0120 - val_accuracy: 0.7903 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "144/144 [==============================] - 32s 225ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.0099 - val_accuracy: 0.7925 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "144/144 [==============================] - 33s 230ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.0071 - val_accuracy: 0.7940 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "144/144 [==============================] - 32s 226ms/step - loss: 8.9063e-04 - accuracy: 1.0000 - val_loss: 1.0072 - val_accuracy: 0.7959 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2607731819152832, 0.8999999761581421]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(test_batches) \n",
    "model.evaluate(imgs, labels, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_second.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "144/144 [==============================] - 58s 400ms/step - loss: 1.6203 - accuracy: 0.7282 - val_loss: 1.2224 - val_accuracy: 0.7483 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "144/144 [==============================] - 32s 225ms/step - loss: 0.0577 - accuracy: 0.9895 - val_loss: 0.7254 - val_accuracy: 0.8306 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "144/144 [==============================] - 32s 226ms/step - loss: 1.3150e-04 - accuracy: 1.0000 - val_loss: 0.6227 - val_accuracy: 0.8637 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "144/144 [==============================] - 32s 224ms/step - loss: 1.3864e-05 - accuracy: 1.0000 - val_loss: 0.6716 - val_accuracy: 0.8643 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "144/144 [==============================] - 32s 221ms/step - loss: 6.6219e-06 - accuracy: 1.0000 - val_loss: 0.6707 - val_accuracy: 0.8660 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history3 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2331136018037796, 0.8999999761581421]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(test_batches) \n",
    "model.evaluate(imgs, labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_second_adam.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [64, 64]\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(vgg.output)\n",
    "prediction = Dense(35, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=vgg.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 35)                71715     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,786,403\n",
      "Trainable params: 71,715\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "144/144 [==============================] - 225s 2s/step - loss: 2.4909 - accuracy: 0.8606 - val_loss: 0.4487 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "144/144 [==============================] - 222s 2s/step - loss: 8.5592e-04 - accuracy: 1.0000 - val_loss: 0.3638 - val_accuracy: 0.9287 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "144/144 [==============================] - 219s 2s/step - loss: 7.3005e-05 - accuracy: 1.0000 - val_loss: 0.3434 - val_accuracy: 0.9334 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "144/144 [==============================] - 222s 2s/step - loss: 3.5255e-05 - accuracy: 1.0000 - val_loss: 0.3316 - val_accuracy: 0.9358 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "144/144 [==============================] - 235s 2s/step - loss: 2.5601e-05 - accuracy: 1.0000 - val_loss: 0.3225 - val_accuracy: 0.9376 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "144/144 [==============================] - 232s 2s/step - loss: 2.0222e-05 - accuracy: 1.0000 - val_loss: 0.3149 - val_accuracy: 0.9390 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "144/144 [==============================] - 227s 2s/step - loss: 1.6914e-05 - accuracy: 1.0000 - val_loss: 0.3083 - val_accuracy: 0.9404 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "144/144 [==============================] - 219s 2s/step - loss: 1.4558e-05 - accuracy: 1.0000 - val_loss: 0.3025 - val_accuracy: 0.9414 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "144/144 [==============================] - 227s 2s/step - loss: 1.2753e-05 - accuracy: 1.0000 - val_loss: 0.2978 - val_accuracy: 0.9424 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "144/144 [==============================] - 234s 2s/step - loss: 1.1310e-05 - accuracy: 1.0000 - val_loss: 0.2926 - val_accuracy: 0.9433 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01515552680939436, 1.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(test_batches) \n",
    "model.evaluate(imgs, labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_second_vgg16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b5871bf28a57d4b9157c5a5a9f61e3b60ec91a012fb12b356be1a8edc8bee2b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
